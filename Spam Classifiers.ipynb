{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifiers\n",
    "\n",
    "Explore text message data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...   spam\n",
       "6  Even my brother is not like to speak with me. ...    ham\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...    ham\n",
       "8  WINNER!! As a valued network customer you have...   spam\n",
       "9  Had your mobile 11 months or more? U R entitle...   spam"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets check what percentage of the documents in `spam_data` are spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sum(spam_data['target'])/len(spam_data['target']))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit the training data `X_train` using a Count Vectorizer with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the longest token in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vect.get_feature_names(),key = len)[-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9716631580734427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_predicted = clf.predict(X_test_transformed)\n",
    "\n",
    "print(roc_auc_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n",
    "\n",
    "###### What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to      2.198406\n",
      "you     2.266493\n",
      "the     2.707383\n",
      "in      2.890761\n",
      "and     2.976764\n",
      "is      3.003012\n",
      "me      3.111530\n",
      "for     3.206840\n",
      "it      3.224384\n",
      "my      3.231044\n",
      "call    3.297812\n",
      "your    3.300196\n",
      "of      3.319473\n",
      "have    3.354130\n",
      "that    3.413811\n",
      "on      3.463136\n",
      "now     3.465949\n",
      "can     3.545053\n",
      "are     3.560414\n",
      "so      3.566625\n",
      "dtype: float64 000pes         8.644919\n",
      "0089           8.644919\n",
      "0121           8.644919\n",
      "01223585236    8.644919\n",
      "0125698789     8.644919\n",
      "02072069400    8.644919\n",
      "02073162414    8.644919\n",
      "02085076972    8.644919\n",
      "021            8.644919\n",
      "0430           8.644919\n",
      "07008009200    8.644919\n",
      "07099833605    8.644919\n",
      "07123456789    8.644919\n",
      "0721072        8.644919\n",
      "07753741225    8.644919\n",
      "077xxx         8.644919\n",
      "078            8.644919\n",
      "07808247860    8.644919\n",
      "07808726822    8.644919\n",
      "078498         8.644919\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "idfs = vectorizer.idf_\n",
    "names_idfs = list(zip(feature_names, idfs))\n",
    "\n",
    "smallest = sorted(names_idfs, key=operator.itemgetter(1))[:20]\n",
    "smallest = pd.Series([features[1] for features in smallest], index=[features[0] for features in smallest])\n",
    "\n",
    "largest = sorted(names_idfs, key=operator.itemgetter(1), reverse=True)[:20]\n",
    "largest = sorted(largest, key=operator.itemgetter(0))\n",
    "largest = pd.Series([features[1] for features in largest], index=[features[0] for features in largest])\n",
    "    \n",
    "print(smallest, largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets play with document frequency\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df = 3).fit(X_train)\n",
    "#feature_names = vect.get_feature_names()\n",
    "X_train_vect = vect.transform(X_train)\n",
    "model  = MultinomialNB(alpha=.1).fit(X_train_vect,y_train)\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "roc_auc_score(y_test,y_pred)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer still has better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's see what is the average length of documents (number of characters) for not spam and spam documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.13284974093264 139.75903614457832\n"
     ]
    }
   ],
   "source": [
    "temp = spam_data.copy()\n",
    "temp['length'] = temp['text'].str.len()\n",
    "average_length = temp.groupby('target')['length'].agg('mean').values\n",
    "print(average_length[0], average_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9661689557407943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_train_transformed_with_length = add_feature(X_train_transformed, X_train.str.len())\n",
    "\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "X_test_transformed_with_length = add_feature(X_test_transformed, X_test.str.len())\n",
    "\n",
    "clf = SVC(C=10000)\n",
    "\n",
    "clf.fit(X_train_transformed_with_length, y_train)\n",
    "\n",
    "y_predicted = clf.predict(X_test_transformed_with_length)\n",
    "\n",
    "print(roc_auc_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Adding average number of digits per document for not spam and spam documents as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2992746113989637 15.759036144578314\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "temp = spam_data.copy()\n",
    "temp['length'] = temp['text'].apply(lambda row:len(re.findall(r'\\d{1}',row)))\n",
    "average_length = temp.groupby('target')['length'].agg('mean').values\n",
    "print(average_length[0], average_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic regression as it works good for sparse matrix\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674528462047772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "temp = spam_data.copy()\n",
    "temp['length_of_doc'] = temp['text'].str.len()\n",
    "temp['digits_count'] = temp['text'].apply(lambda row: len(re.findall(r'(\\d)', row)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1, 3)).fit(X_train['text'])\n",
    "X_train_vectorized = vect.transform(X_train['text'])\n",
    "X_test_vectorized = vect.transform(X_test['text'])\n",
    "X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "X_train_vectorized = add_feature(X_train_vectorized, X_train['digits_count'])\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test['digits_count'])\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "y_score = clf.predict(X_test_vectorized)\n",
    "score = roc_auc_score(y_test, y_score)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding word/ non word charater as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.339274611398963 29.49263721552878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp = spam_data.copy()\n",
    "temp['length'] = temp['text'].apply(lambda row:len(re.findall(r'\\W{1}',row)))\n",
    "average_length = temp.groupby('target')['length'].agg('mean').values\n",
    "\n",
    "print(average_length[0], average_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also lets **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9813973821367333 ['..', '. ', ' i', ' go', ' y', '? ', 'pe', 'go', ' h', 'ca'] ['digit_count', 'ne', 'co', 'ia', 'ar', 'ww', ' r', ' ch', ' x', 'xt']\n"
     ]
    }
   ],
   "source": [
    "temp = spam_data.copy()\n",
    "temp['length_of_doc'] = temp['text'].str.len()\n",
    "temp['digit_count'] = spam_data['text'].apply(lambda row: len(re.findall(r'\\d', row)))\n",
    "temp['non_word_char_count'] = temp['text'].apply(lambda row: len(re.findall(r'\\W', row)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train['text'])\n",
    "X_train_vectorized = vect.transform(X_train['text'])\n",
    "X_test_vectorized = vect.transform(X_test['text'])\n",
    "X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "X_train_vectorized = add_feature(X_train_vectorized, X_train['digit_count'])\n",
    "X_train_vectorized = add_feature(X_train_vectorized, X_train['non_word_char_count'])\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test['digit_count'])\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test['non_word_char_count'])\n",
    "clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "y_score = clf.predict(X_test_vectorized)\n",
    "score = roc_auc_score(y_test, y_score)\n",
    "\n",
    "feature_names = np.append(np.array(vect.get_feature_names()), ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "largest_coefs = feature_names[sorted_coef_index[:-11:-1]]\n",
    "smallest_coefs = feature_names[sorted_coef_index[:10]]\n",
    "    \n",
    "print(score, list(smallest_coefs), list(largest_coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We achieved an ROC score of >98% \n",
    "\n",
    "Using \n",
    "- Count Vectorizer \n",
    "- document frequency lower than **5**\n",
    "- n-grams from n=2 to n=5\n",
    "- analyzer='char_wb'\n",
    "- Logistic Regression with regularization C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
